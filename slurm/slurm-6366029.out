Global seed set to 23
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:104: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:45: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v1.7. Please pass `Trainer.fit(ckpt_path=)` directly instead.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Global seed set to 23
Global seed set to 23
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:104: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
Global seed set to 23
initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:104: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
Global seed set to 23
initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Global seed set to 23
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1905: LightningDeprecationWarning: `trainer.resume_from_checkpoint` is deprecated in v1.5 and will be removed in v1.7. Specify the fit checkpoint path with `trainer.fit(ckpt_path=)` instead.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:275: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:284: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:291: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
Global seed set to 23
initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:104: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.
  rank_zero_deprecation(
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:286: LightningDeprecationWarning: Passing `Trainer(accelerator='ddp')` has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy='ddp')` instead.
  rank_zero_deprecation(
Global seed set to 23
initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
Restoring states from the checkpoint path at logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints/last.ckpt
Running on GPUs 0,1,2,3,
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.86 M params.
Keeping EMAs of 340.
making attention of type 'none' with 512 in_channels
Working with z of shape (1, 3, 32, 32) = 3072 dimensions.
making attention of type 'none' with 512 in_channels
Restored from models/first_stage_models/vq-f8-n256/model.ckpt with 0 missing and 75 unexpected keys
Using first stage also as cond stage.
Monitoring val/loss as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss', 'save_top_k': 3}}
#### Data #####
train, TexQuadTrain, 100000
validation, TexQuadValidation, 2000
accumulate_grad_batches = 1
Setting learning rate to 1.28e-02 = 1 (accumulate_grad_batches) * 4 (num_gpus) * 64 (batchsize) * 5.00e-05 (base_lr)
Running on GPUs 0,1,2,3,
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.86 M params.
Keeping EMAs of 340.
making attention of type 'none' with 512 in_channels
Working with z of shape (1, 3, 32, 32) = 3072 dimensions.
making attention of type 'none' with 512 in_channels
Restored from models/first_stage_models/vq-f8-n256/model.ckpt with 0 missing and 75 unexpected keys
Using first stage also as cond stage.
Monitoring val/loss as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss', 'save_top_k': 3}}
#### Data #####
train, TexQuadTrain, 100000
validation, TexQuadValidation, 2000
accumulate_grad_batches = 1
Setting learning rate to 1.28e-02 = 1 (accumulate_grad_batches) * 4 (num_gpus) * 64 (batchsize) * 5.00e-05 (base_lr)
Running on GPUs 0,1,2,3,
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.86 M params.
Keeping EMAs of 340.
making attention of type 'none' with 512 in_channels
Working with z of shape (1, 3, 32, 32) = 3072 dimensions.
making attention of type 'none' with 512 in_channels
Restored from models/first_stage_models/vq-f8-n256/model.ckpt with 0 missing and 75 unexpected keys
Using first stage also as cond stage.
Monitoring val/loss as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss', 'save_top_k': 3}}
#### Data #####
train, TexQuadTrain, 100000
validation, TexQuadValidation, 2000
accumulate_grad_batches = 1
Setting learning rate to 1.28e-02 = 1 (accumulate_grad_batches) * 4 (num_gpus) * 64 (batchsize) * 5.00e-05 (base_lr)
Traceback (most recent call last):
  File "/gscratch/realitylab/yifan1/latent-diffusion/main.py", line 719, in <module>
    trainer.fit(model, data)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1142, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1108, in _restore_modules_and_callbacks
    self.checkpoint_connector.resume_start(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 79, in resume_start
    self._loaded_checkpoint = self._load_and_validate_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 83, in _load_and_validate_checkpoint
    loaded_checkpoint = self.trainer.training_type_plugin.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 190, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py", line 66, in load_checkpoint
    raise FileNotFoundError(f"Checkpoint at {path} not found. Aborting training.")
FileNotFoundError: Checkpoint at logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints/last.ckpt not found. Aborting training.
Traceback (most recent call last):
  File "/gscratch/realitylab/yifan1/latent-diffusion/main.py", line 719, in <module>
    trainer.fit(model, data)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1142, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1108, in _restore_modules_and_callbacks
    self.checkpoint_connector.resume_start(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 79, in resume_start
    self._loaded_checkpoint = self._load_and_validate_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 83, in _load_and_validate_checkpoint
    loaded_checkpoint = self.trainer.training_type_plugin.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 190, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py", line 66, in load_checkpoint
    raise FileNotFoundError(f"Checkpoint at {path} not found. Aborting training.")
FileNotFoundError: Checkpoint at logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints/last.ckpt not found. Aborting training.
Traceback (most recent call last):
  File "/gscratch/realitylab/yifan1/latent-diffusion/main.py", line 719, in <module>
    trainer.fit(model, data)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1142, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1108, in _restore_modules_and_callbacks
    self.checkpoint_connector.resume_start(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 79, in resume_start
    self._loaded_checkpoint = self._load_and_validate_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 83, in _load_and_validate_checkpoint
    loaded_checkpoint = self.trainer.training_type_plugin.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 190, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py", line 66, in load_checkpoint
    raise FileNotFoundError(f"Checkpoint at {path} not found. Aborting training.")
FileNotFoundError: Checkpoint at logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints/last.ckpt not found. Aborting training.
/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:510: UserWarning: Error handling mechanism for deadlock detection is uninitialized. Skipping check.
  rank_zero_warn("Error handling mechanism for deadlock detection is uninitialized. Skipping check.")
Running on GPUs 0,1,2,3,
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 274.86 M params.
Keeping EMAs of 340.
making attention of type 'none' with 512 in_channels
Working with z of shape (1, 3, 32, 32) = 3072 dimensions.
making attention of type 'none' with 512 in_channels
Restored from models/first_stage_models/vq-f8-n256/model.ckpt with 0 missing and 75 unexpected keys
Using first stage also as cond stage.
Monitoring val/loss as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss', 'save_top_k': 3}}
#### Data #####
train, TexQuadTrain, 100000
validation, TexQuadValidation, 2000
accumulate_grad_batches = 1
Setting learning rate to 1.28e-02 = 1 (accumulate_grad_batches) * 4 (num_gpus) * 64 (batchsize) * 5.00e-05 (base_lr)
Summoning checkpoint.

Traceback (most recent call last):
  File "main.py", line 719, in <module>
    trainer.fit(model, data)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 740, in fit
    self._call_and_handle_interrupt(
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 685, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 777, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1142, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1108, in _restore_modules_and_callbacks
    self.checkpoint_connector.resume_start(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 79, in resume_start
    self._loaded_checkpoint = self._load_and_validate_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 83, in _load_and_validate_checkpoint
    loaded_checkpoint = self.trainer.training_type_plugin.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 190, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/mmfs1/home/yifan1/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/io/torch_plugin.py", line 66, in load_checkpoint
    raise FileNotFoundError(f"Checkpoint at {path} not found. Aborting training.")
FileNotFoundError: Checkpoint at logs/2022-10-07T00-47-19_tex-ldm-vq-f8-ipt/checkpoints/last.ckpt not found. Aborting training.
